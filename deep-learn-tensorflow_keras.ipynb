{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow==2.0.0-rc1\n",
    "#!pip install tensorflow-gpu==2.0.0-rc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) In this task, you'll build an ANN and train and test it using the MNIST data. This ANN should consist of two hidden layers and one output layer. All of the hidden layers should be dense. The first layer and the second layer should have neuron sizes of 32 and 16, respectively. Train this model for 20 epochs, and compare your training and test set performance with the example in the checkpoint. Is there any difference? If so, why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, load the data and do the preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "input_dim = 784  # 28*28\n",
    "output_dim = nb_classes = 10\n",
    "batch_size = 128\n",
    "nb_epoch = 20\n",
    "\n",
    "X_train = X_train.reshape(60000, input_dim)\n",
    "X_test = X_test.reshape(10000, input_dim)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, one-hot code your target variable using the `to_categorical()` function from the `keras.utils` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "Y_train = to_categorical(y_train, nb_classes)\n",
    "Y_test = to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check the size of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the size of each image is 784. In fact, all images in *MNIST* are 28 by 28 pixels, and 784 is just the result of multiplying 28 by 28. So, the data that you have is a flattened version of the images, where each row in the 28x28 matrix is concatenated side by side. \n",
    "\n",
    "Now, plot some images and see what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(141)\n",
    "plt.imshow(X_train[123].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[123]))\n",
    "\n",
    "plt.subplot(142)\n",
    "plt.imshow(X_train[124].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[124]))\n",
    "\n",
    "plt.subplot(143)\n",
    "plt.imshow(X_train[125].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[125]))\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.imshow(X_train[126].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[126]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're now ready to jump into building the ANN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model\n",
    "\n",
    "As mentioned before, you'll build your model using the `Sequential()` class of the `keras.models` module. First, create your model as follows:\n",
    "```python\n",
    "model = Sequential()\n",
    "```\n",
    "Then you can start to add layers to your `model` object one by one (that is, sequentially). The layer type that you'll use is called the dense layer, and you can import it from the `keras.layers` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "# The first dense layer\n",
    "model.add(Dense(32, input_shape=(784,), activation=\"relu\"))\n",
    "# The second dense layer\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "# The last layer is the output layer\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that above, the neuron size of the output layer is set to `10`. This is because *MNIST* has 10 classes. The code also sets the activation function of the output layer to `softmax`. Later, you'll explore why softmax is used as the activation function in the output layer. But for now, know that when you give an image as an input to the model, your model will produce 10 probabilities for each of the 10 classes in the *MNIST* data. The largest probability class will be the prediction of the model.\n",
    "\n",
    "You can look at the structure of your ANN model using the `summary()` method of your `model` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 25,818\n",
      "Trainable params: 25,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "\n",
    "Now you can compile your model. When compiling the model, define three things:\n",
    "\n",
    "1. **The optimizer that will be used in the training:** If you don't know about optimizers in deep learning, don't worry. You just need to use one in this checkpoint, and you'll learn about optimizers later in this module.\n",
    "2. **The loss function:** It's necessary to specify a loss function for a model. Training algorithms use this loss function and try to minimize it during the training. This will also be covered in the next checkpoint.\n",
    "3. **The metric to measure the training performance of your model:** In this example, you use the accuracy metric, because your task is a classification task and your dataset is a balanced one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "You're now ready to train the model. Training a model in Keras is done by calling the `fit()` method of the `model` object. In the following, you train your model using the following specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000022F8B936948> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000022F8B936948> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "60000/60000 [==============================] - 1s 18us/sample - loss: 1.7496 - accuracy: 0.4234\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.7188 - accuracy: 0.8135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.4725 - accuracy: 0.8683\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.3987 - accuracy: 0.8880\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.3606 - accuracy: 0.8979\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.3361 - accuracy: 0.9043\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.3191 - accuracy: 0.9089\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.3054 - accuracy: 0.9127\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.2942 - accuracy: 0.9161\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.2844 - accuracy: 0.9186\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.2756 - accuracy: 0.9211\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.2676 - accuracy: 0.9240\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.2603 - accuracy: 0.9255\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.2534 - accuracy: 0.9272\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.2469 - accuracy: 0.9297\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.2405 - accuracy: 0.9317\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.2348 - accuracy: 0.9332\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.2293 - accuracy: 0.9344\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.2239 - accuracy: 0.9364\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.2188 - accuracy: 0.9380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22f8b94ae48>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting `verbose=1` prints out some results after each epoch\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "\n",
    "The last step is to evaluate the model using the test set that you set apart before. For this purpose, use the `evaluate()` method of the `model` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.21276673160791398\n",
      "Test accuracy: 0.9378\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It has a little bit less accuracy than the example model (94% vs 97%), probably because it has less neurons in the model layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) In this task, build another ANN. This ANN should have five hidden layers and one output layer. All of the layers should be dense. The neuron numbers for the hidden layers should be 1024, 512, 256, 128, and 64. Train this model for 20 epochs, and test it using the same data from the previous task. Then compare your results. Is there any difference? If so, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "input_dim = 784  # 28*28\n",
    "output_dim = nb_classes = 10\n",
    "batch_size = 128\n",
    "nb_epoch = 20\n",
    "\n",
    "X_train = X_train.reshape(60000, input_dim)\n",
    "X_test = X_test.reshape(10000, input_dim)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, one-hot code your target variable using the `to_categorical()` function from the `keras.utils` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "Y_train = to_categorical(y_train, nb_classes)\n",
    "Y_test = to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check the size of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model\n",
    "\n",
    "As mentioned before, you'll build your model using the `Sequential()` class of the `keras.models` module. First, create your model as follows:\n",
    "```python\n",
    "model = Sequential()\n",
    "```\n",
    "Then you can start to add layers to your `model` object one by one (that is, sequentially). The layer type that you'll use is called the dense layer, and you can import it from the `keras.layers` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "# The first dense layer\n",
    "model.add(Dense(1024, input_shape=(784,), activation=\"relu\"))\n",
    "# The second dense layer\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "# The third dense layer\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "# The fourth dense layer\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "# The fourth dense layer\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "# The last layer is the output layer\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that above, the neuron size of the output layer is set to `10`. This is because *MNIST* has 10 classes. The code also sets the activation function of the output layer to `softmax`. Later, you'll explore why softmax is used as the activation function in the output layer. But for now, know that when you give an image as an input to the model, your model will produce 10 probabilities for each of the 10 classes in the *MNIST* data. The largest probability class will be the prediction of the model.\n",
    "\n",
    "You can look at the structure of your ANN model using the `summary()` method of your `model` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 1024)              803840    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 1,501,770\n",
      "Trainable params: 1,501,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "\n",
    "Now you can compile your model. When compiling the model, define three things:\n",
    "\n",
    "1. **The optimizer that will be used in the training:** If you don't know about optimizers in deep learning, don't worry. You just need to use one in this checkpoint, and you'll learn about optimizers later in this module.\n",
    "2. **The loss function:** It's necessary to specify a loss function for a model. Training algorithms use this loss function and try to minimize it during the training. This will also be covered in the next checkpoint.\n",
    "3. **The metric to measure the training performance of your model:** In this example, you use the accuracy metric, because your task is a classification task and your dataset is a balanced one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "You're now ready to train the model. Training a model in Keras is done by calling the `fit()` method of the `model` object. In the following, you train your model using the following specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000022F8BBA11F8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000022F8BBA11F8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1.2588 - accuracy: 0.6668\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.3771 - accuracy: 0.8944\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.2813 - accuracy: 0.9195\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.2363 - accuracy: 0.9313\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.2038 - accuracy: 0.9412ETA: 0s - loss: 0.2046 - \n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.1806 - accuracy: 0.9476\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.1612 - accuracy: 0.9531\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 0.1450 - accuracy: 0.9578\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.1318 - accuracy: 0.9617\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.1205 - accuracy: 0.9653\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.1102 - accuracy: 0.9683\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.1011 - accuracy: 0.9711\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.0927 - accuracy: 0.9731\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.0864 - accuracy: 0.9756\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 0.0805 - accuracy: 0.9770\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 5s 92us/sample - loss: 0.0738 - accuracy: 0.9791\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.0691 - accuracy: 0.9805\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.0636 - accuracy: 0.9825\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.0592 - accuracy: 0.9830\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 5s 92us/sample - loss: 0.0552 - accuracy: 0.9844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22f8f182208>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting `verbose=1` prints out some results after each epoch\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "\n",
    "The last step is to evaluate the model using the test set that you set apart before. For this purpose, use the `evaluate()` method of the `model` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.0810042532064952\n",
      "Test accuracy: 0.9737\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Much better accuracy with this model (almost 98%), more layers and more neurons per layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
