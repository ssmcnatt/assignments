{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 540,
     "status": "ok",
     "timestamp": 1605822280965,
     "user": {
      "displayName": "Steve McNatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_xqDao3VTsC8rTyyomoAZzFcCZbR3MyZGQ1aMmQ=s64",
      "userId": "17122454985167578805"
     },
     "user_tz": 360
    },
    "id": "r6guOKpdbcb2"
   },
   "outputs": [],
   "source": [
    "#!pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2906,
     "status": "ok",
     "timestamp": 1605822283342,
     "user": {
      "displayName": "Steve McNatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_xqDao3VTsC8rTyyomoAZzFcCZbR3MyZGQ1aMmQ=s64",
      "userId": "17122454985167578805"
     },
     "user_tz": 360
    },
    "id": "Ouylzvl3bcb6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06WhosNAbcb9"
   },
   "source": [
    "## You can use any data that you like. There are many available text datasets on Kaggle, and you can use them if you like. Or, if you have access to substantial computing power, you can also use large collections of text. But it's a good idea to pick a modest-size dataset so that you can experiment a lot without waiting too much on running the computations.\n",
    "\n",
    "## Your dataset can include nontext data along with some text data. However, having some text data in your dataset is obligatory.\n",
    "\n",
    "## In your project, do the following:\n",
    "\n",
    "## 1) First, make sure to clean your data. You can use the data-cleaning techniques that you practiced in the Text preprocessing checkpoint. But keep in mind that every text dataset is likely to have its own particular cleaning requirements. Make sure to clean your dataset appropriately.\n",
    "\n",
    "## 2) If your dataset includes numerical features, then work on them for cleaning purposes; for example, deal with any missing values and outliers.\n",
    "\n",
    "## 3) After the data-cleaning step, do some exploratory data analysis to get to know your dataset better. In the exploratory data analysis, analyze your numerical features as well as your text features. If you feel that converting your text features into a numerical form is required for your exploratory analysis, then you can do this step after vectorizing your text.\n",
    "\n",
    "## 4) Convert your text features into numerical form. You're free to choose your method. You can use BoW, TF-IDF, word2vec, or any other method that you'd like to experiment with. It's a good idea to use several methods so that you can compare the results with respect to the methods that you apply.\n",
    "\n",
    "## 5) You can choose whatever NLP task you want. You're completely free in your choice. Here are some possible options:\n",
    "* Sentiment analysis\n",
    "* Text classification\n",
    "* Topic modeling\n",
    "* Developing a more sophisticated chatbot than what you've already developed in this module\n",
    "* Training a deep-learning model for a supervised or unsupervised task of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYwqggvMlPJO"
   },
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBbErtSMlV0f"
   },
   "source": [
    "## IMDB movie reviews: \n",
    "https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4544,
     "status": "ok",
     "timestamp": 1605822284988,
     "user": {
      "displayName": "Steve McNatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_xqDao3VTsC8rTyyomoAZzFcCZbR3MyZGQ1aMmQ=s64",
      "userId": "17122454985167578805"
     },
     "user_tz": 360
    },
    "id": "8LARj0Pzbcb-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imdb = pd.read_csv('data/IMDB Dataset.csv')\n",
    "imdb.columns = ['review', '-sentiment-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 4538,
     "status": "ok",
     "timestamp": 1605822284990,
     "user": {
      "displayName": "Steve McNatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_xqDao3VTsC8rTyyomoAZzFcCZbR3MyZGQ1aMmQ=s64",
      "userId": "17122454985167578805"
     },
     "user_tz": 360
    },
    "id": "HUAjDST-bccB",
    "outputId": "a3c9147d-c1e9-4a25-8bb9-6ced8ec4e2f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>-sentiment-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review -sentiment-\n",
       "0  One of the other reviewers has mentioned that ...    positive\n",
       "1  A wonderful little production. <br /><br />The...    positive\n",
       "2  I thought this was a wonderful way to spend ti...    positive\n",
       "3  Basically there's a family where a little boy ...    negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...    positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4534,
     "status": "ok",
     "timestamp": 1605822284992,
     "user": {
      "displayName": "Steve McNatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_xqDao3VTsC8rTyyomoAZzFcCZbR3MyZGQ1aMmQ=s64",
      "userId": "17122454985167578805"
     },
     "user_tz": 360
    },
    "id": "4KHiyfnQrZjK",
    "outputId": "da36a233-837c-4efb-bade-5e1acb7a251d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   review       50000 non-null  object\n",
      " 1   -sentiment-  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "imdb.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4531,
     "status": "ok",
     "timestamp": 1605822284993,
     "user": {
      "displayName": "Steve McNatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_xqDao3VTsC8rTyyomoAZzFcCZbR3MyZGQ1aMmQ=s64",
      "userId": "17122454985167578805"
     },
     "user_tz": 360
    },
    "id": "KwqgpwnIj-WH"
   },
   "outputs": [],
   "source": [
    "def clean_text(doc):\n",
    "    # tokenize\n",
    "    doc_words = word_tokenize(doc)\n",
    "\n",
    "    # lowercase\n",
    "    doc_lower = [word.lower() for word in doc_words]\n",
    "\n",
    "    # remove stop words\n",
    "    doc_nostop = [token for token in doc_lower if token not in stopwords.words('english')]\n",
    "\n",
    "    # remove punctuation\n",
    "    doc_nopunc = [word for word in doc_nostop if word.isalpha()]\n",
    "\n",
    "    # stem the tokens\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    doc_stem = [stemmer.stem(word) for word in doc_nopunc]\n",
    "\n",
    "    return_doc = ' '.join(doc_stem)\n",
    "\n",
    "    return return_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4520,
     "status": "ok",
     "timestamp": 1605822284996,
     "user": {
      "displayName": "Steve McNatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_xqDao3VTsC8rTyyomoAZzFcCZbR3MyZGQ1aMmQ=s64",
      "userId": "17122454985167578805"
     },
     "user_tz": 360
    },
    "id": "VR1eh59lUiZJ"
   },
   "outputs": [],
   "source": [
    "# Split into positive and negative documents\n",
    "pos_doc = ''\n",
    "neg_doc = ''\n",
    "for index, row in imdb[:2000].iterrows():\n",
    "    if row['-sentiment-'] == 'positive':\n",
    "        pos_doc = pos_doc + row['review']\n",
    "    else:\n",
    "        neg_doc = neg_doc + row['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 4518,
     "status": "ok",
     "timestamp": 1605822284997,
     "user": {
      "displayName": "Steve McNatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_xqDao3VTsC8rTyyomoAZzFcCZbR3MyZGQ1aMmQ=s64",
      "userId": "17122454985167578805"
     },
     "user_tz": 360
    },
    "id": "QMmwkB-mX6yC"
   },
   "source": [
    "## Perform NLP, parse, and clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 83259,
     "status": "ok",
     "timestamp": 1605822363741,
     "user": {
      "displayName": "Steve McNatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_xqDao3VTsC8rTyyomoAZzFcCZbR3MyZGQ1aMmQ=s64",
      "userId": "17122454985167578805"
     },
     "user_tz": 360
    },
    "id": "4YeJi4qXhEIw"
   },
   "outputs": [],
   "source": [
    "# Parse the reviews\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 2000000\n",
    "pos_doc = nlp(pos_doc)\n",
    "neg_doc = nlp(neg_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 83256,
     "status": "ok",
     "timestamp": 1605822363743,
     "user": {
      "displayName": "Steve McNatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_xqDao3VTsC8rTyyomoAZzFcCZbR3MyZGQ1aMmQ=s64",
      "userId": "17122454985167578805"
     },
     "user_tz": 360
    },
    "id": "3YAippcBbcci"
   },
   "outputs": [],
   "source": [
    "# Group into sentences\n",
    "pos_sents = [[sent, \"positive\"] for sent in pos_doc.sents]\n",
    "neg_sents = [[sent, \"negative\"] for sent in neg_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two docs into one DataFrame\n",
    "sentences = pd.DataFrame(pos_sents[:2000] + neg_sents[:2000], columns = [\"text\", \"-sentiment-\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 83253,
     "status": "ok",
     "timestamp": 1605822363745,
     "user": {
      "displayName": "Steve McNatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_xqDao3VTsC8rTyyomoAZzFcCZbR3MyZGQ1aMmQ=s64",
      "userId": "17122454985167578805"
     },
     "user_tz": 360
    },
    "id": "09MIDqzlbUjk",
    "outputId": "e05c04e0-ba9e-4bfd-962b-7e99007789eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>-sentiment-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(One, of, the, other, reviewers, has, mentione...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(They, are, right, ,, as, this, is, exactly, w...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(first, thing, that, struck, me, about, Oz, wa...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Trust, me, ,, this, is, not, a, show, for, th...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(This, show, pulls, no, punches, with, regards...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text -sentiment-\n",
       "0  (One, of, the, other, reviewers, has, mentione...    positive\n",
       "1  (They, are, right, ,, as, this, is, exactly, w...    positive\n",
       "2  (first, thing, that, struck, me, about, Oz, wa...    positive\n",
       "3  (Trust, me, ,, this, is, not, a, show, for, th...    positive\n",
       "4  (This, show, pulls, no, punches, with, regards...    positive"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 86904,
     "status": "ok",
     "timestamp": 1605822367406,
     "user": {
      "displayName": "Steve McNatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_xqDao3VTsC8rTyyomoAZzFcCZbR3MyZGQ1aMmQ=s64",
      "userId": "17122454985167578805"
     },
     "user_tz": 360
    },
    "id": "4UE03PL2ZQT6"
   },
   "outputs": [],
   "source": [
    "# clean the text\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    sentences.loc[i, \"text\"] = \" \".join(\n",
    "        [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop and token.is_alpha])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWteTtHITSVa"
   },
   "source": [
    "## Use BOW to convert text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 106429,
     "status": "ok",
     "timestamp": 1605822386934,
     "user": {
      "displayName": "Steve McNatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_xqDao3VTsC8rTyyomoAZzFcCZbR3MyZGQ1aMmQ=s64",
      "userId": "17122454985167578805"
     },
     "user_tz": 360
    },
    "id": "NdNw9pdSb73i"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "X = vectorizer.fit_transform(sentences[\"text\"])\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "bow_sentences = pd.concat([bow_df, sentences[[\"text\", \"-sentiment-\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 106428,
     "status": "ok",
     "timestamp": 1605822386938,
     "user": {
      "displayName": "Steve McNatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_xqDao3VTsC8rTyyomoAZzFcCZbR3MyZGQ1aMmQ=s64",
      "userId": "17122454985167578805"
     },
     "user_tz": 360
    },
    "id": "PsHOHggDZTf6",
    "outputId": "3b1f105a-83b5-4366-c0d1-d01b9507a86c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 7219)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_sentences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the BOW data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-eU1Tz2YcruD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "Training set score: 0.9084375\n",
      "\n",
      "Test set score: 0.67\n",
      "----------------------Random Forest Scores----------------------\n",
      "Training set score: 0.9534375\n",
      "\n",
      "Test set score: 0.64375\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "Training set score: 0.695625\n",
      "\n",
      "Test set score: 0.5925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = bow_sentences['-sentiment-']\n",
    "X = np.array(bow_sentences.drop(['text','-sentiment-'], 1))\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Models\n",
    "lr = LogisticRegression()\n",
    "rfc = RandomForestClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "rfc.fit(X_train, y_train)\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "print(\"----------------------Logistic Regression Scores----------------------\")\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Random Forest Scores----------------------\")\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
    "print('Training set score:', gbc.score(X_train, y_train))\n",
    "print('\\nTest set score:', gbc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWteTtHITSVa"
   },
   "source": [
    "## Use TF-IDF to convert text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 106429,
     "status": "ok",
     "timestamp": 1605822386934,
     "user": {
      "displayName": "Steve McNatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_xqDao3VTsC8rTyyomoAZzFcCZbR3MyZGQ1aMmQ=s64",
      "userId": "17122454985167578805"
     },
     "user_tz": 360
    },
    "id": "NdNw9pdSb73i"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5, min_df=2, use_idf=True, norm=u'l2', smooth_idf=True)\n",
    "X = vectorizer.fit_transform(sentences[\"text\"])\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "tfidf_sentences = pd.concat([bow_df, sentences[[\"text\", \"-sentiment-\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 106428,
     "status": "ok",
     "timestamp": 1605822386938,
     "user": {
      "displayName": "Steve McNatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_xqDao3VTsC8rTyyomoAZzFcCZbR3MyZGQ1aMmQ=s64",
      "userId": "17122454985167578805"
     },
     "user_tz": 360
    },
    "id": "PsHOHggDZTf6",
    "outputId": "3b1f105a-83b5-4366-c0d1-d01b9507a86c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abbot</th>\n",
       "      <th>abbreviate</th>\n",
       "      <th>abet</th>\n",
       "      <th>abhorrent</th>\n",
       "      <th>abide</th>\n",
       "      <th>ability</th>\n",
       "      <th>abject</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>ziyi</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zulu</th>\n",
       "      <th>zwick</th>\n",
       "      <th>text</th>\n",
       "      <th>-sentiment-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>reviewer mention watch Oz episode hook</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>right exactly happen</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>thing strike Oz brutality unflinche scene viol...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>trust faint hearted timid</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pull punch regard drug sex violence</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7219 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaliyah  abandon  abbot  abbreviate  abet  abhorrent  abide  ability  \\\n",
       "0        0        0      0           0     0          0      0        0   \n",
       "1        0        0      0           0     0          0      0        0   \n",
       "2        0        0      0           0     0          0      0        0   \n",
       "3        0        0      0           0     0          0      0        0   \n",
       "4        0        0      0           0     0          0      0        0   \n",
       "\n",
       "   abject  able  ...  ziyi  zombie  zombies  zone  zoo  zoom  zulu  zwick  \\\n",
       "0       0     0  ...     0       0        0     0    0     0     0      0   \n",
       "1       0     0  ...     0       0        0     0    0     0     0      0   \n",
       "2       0     0  ...     0       0        0     0    0     0     0      0   \n",
       "3       0     0  ...     0       0        0     0    0     0     0      0   \n",
       "4       0     0  ...     0       0        0     0    0     0     0      0   \n",
       "\n",
       "                                                text  -sentiment-  \n",
       "0             reviewer mention watch Oz episode hook     positive  \n",
       "1                               right exactly happen     positive  \n",
       "2  thing strike Oz brutality unflinche scene viol...     positive  \n",
       "3                          trust faint hearted timid     positive  \n",
       "4                pull punch regard drug sex violence     positive  \n",
       "\n",
       "[5 rows x 7219 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the TF-IDF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-eU1Tz2YcruD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "Training set score: 0.9179166666666667\n",
      "\n",
      "Test set score: 0.645\n",
      "----------------------Random Forest Scores----------------------\n",
      "Training set score: 0.9541666666666667\n",
      "\n",
      "Test set score: 0.613125\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "Training set score: 0.7079166666666666\n",
      "\n",
      "Test set score: 0.58625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = tfidf_sentences['-sentiment-']\n",
    "X = np.array(tfidf_sentences.drop(['text','-sentiment-'], 1))\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)\n",
    "\n",
    "# Models\n",
    "lr = LogisticRegression()\n",
    "rfc = RandomForestClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "rfc.fit(X_train, y_train)\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "print(\"----------------------Logistic Regression Scores----------------------\")\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Random Forest Scores----------------------\")\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
    "print('Training set score:', gbc.score(X_train, y_train))\n",
    "print('\\nTest set score:', gbc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Word2Vec to convert text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Train word2vec on the sentences\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences[\"text\"],\n",
    "    workers=4,\n",
    "    min_count=0.5,\n",
    "    window=12,\n",
    "    sg=0,\n",
    "    sample=0.001,\n",
    "    size=100,\n",
    "    hs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-sentiment-</th>\n",
       "      <th>text</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>reviewer mention watch Oz episode hook</td>\n",
       "      <td>-0.074249</td>\n",
       "      <td>0.114671</td>\n",
       "      <td>0.015121</td>\n",
       "      <td>0.073763</td>\n",
       "      <td>0.069884</td>\n",
       "      <td>0.008495</td>\n",
       "      <td>0.053634</td>\n",
       "      <td>0.012058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178076</td>\n",
       "      <td>0.024288</td>\n",
       "      <td>-0.016427</td>\n",
       "      <td>0.016274</td>\n",
       "      <td>-0.063890</td>\n",
       "      <td>-0.139000</td>\n",
       "      <td>0.106942</td>\n",
       "      <td>0.036321</td>\n",
       "      <td>-0.016587</td>\n",
       "      <td>-0.087598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>right exactly happen</td>\n",
       "      <td>-0.066062</td>\n",
       "      <td>0.178012</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.106561</td>\n",
       "      <td>0.080411</td>\n",
       "      <td>-0.014588</td>\n",
       "      <td>0.107324</td>\n",
       "      <td>-0.001012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148205</td>\n",
       "      <td>0.034288</td>\n",
       "      <td>-0.035965</td>\n",
       "      <td>0.033020</td>\n",
       "      <td>-0.022341</td>\n",
       "      <td>-0.099260</td>\n",
       "      <td>0.208515</td>\n",
       "      <td>0.149304</td>\n",
       "      <td>-0.057306</td>\n",
       "      <td>-0.186135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>thing strike Oz brutality unflinche scene viol...</td>\n",
       "      <td>-0.069869</td>\n",
       "      <td>0.162468</td>\n",
       "      <td>0.028621</td>\n",
       "      <td>0.098696</td>\n",
       "      <td>0.072301</td>\n",
       "      <td>-0.001498</td>\n",
       "      <td>0.076688</td>\n",
       "      <td>0.024982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143718</td>\n",
       "      <td>0.040915</td>\n",
       "      <td>-0.018511</td>\n",
       "      <td>0.028187</td>\n",
       "      <td>-0.019662</td>\n",
       "      <td>-0.100381</td>\n",
       "      <td>0.110346</td>\n",
       "      <td>0.068416</td>\n",
       "      <td>-0.044475</td>\n",
       "      <td>-0.107071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>trust faint hearted timid</td>\n",
       "      <td>-0.127443</td>\n",
       "      <td>0.218737</td>\n",
       "      <td>0.056772</td>\n",
       "      <td>0.129232</td>\n",
       "      <td>0.085079</td>\n",
       "      <td>-0.003635</td>\n",
       "      <td>0.079976</td>\n",
       "      <td>0.024315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119139</td>\n",
       "      <td>0.043489</td>\n",
       "      <td>-0.012940</td>\n",
       "      <td>0.025341</td>\n",
       "      <td>0.014246</td>\n",
       "      <td>-0.057026</td>\n",
       "      <td>0.168648</td>\n",
       "      <td>0.102968</td>\n",
       "      <td>-0.070157</td>\n",
       "      <td>-0.163152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>pull punch regard drug sex violence</td>\n",
       "      <td>-0.007807</td>\n",
       "      <td>0.150145</td>\n",
       "      <td>-0.003704</td>\n",
       "      <td>0.085559</td>\n",
       "      <td>0.072203</td>\n",
       "      <td>-0.012184</td>\n",
       "      <td>0.108270</td>\n",
       "      <td>0.005073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142472</td>\n",
       "      <td>0.040177</td>\n",
       "      <td>-0.026639</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>-0.023089</td>\n",
       "      <td>-0.104056</td>\n",
       "      <td>0.151252</td>\n",
       "      <td>0.110283</td>\n",
       "      <td>-0.047110</td>\n",
       "      <td>-0.138467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  -sentiment-                                               text         0  \\\n",
       "0    positive             reviewer mention watch Oz episode hook -0.074249   \n",
       "1    positive                               right exactly happen -0.066062   \n",
       "2    positive  thing strike Oz brutality unflinche scene viol... -0.069869   \n",
       "3    positive                          trust faint hearted timid -0.127443   \n",
       "4    positive                pull punch regard drug sex violence -0.007807   \n",
       "\n",
       "          1         2         3         4         5         6         7  ...  \\\n",
       "0  0.114671  0.015121  0.073763  0.069884  0.008495  0.053634  0.012058  ...   \n",
       "1  0.178012  0.008500  0.106561  0.080411 -0.014588  0.107324 -0.001012  ...   \n",
       "2  0.162468  0.028621  0.098696  0.072301 -0.001498  0.076688  0.024982  ...   \n",
       "3  0.218737  0.056772  0.129232  0.085079 -0.003635  0.079976  0.024315  ...   \n",
       "4  0.150145 -0.003704  0.085559  0.072203 -0.012184  0.108270  0.005073  ...   \n",
       "\n",
       "         90        91        92        93        94        95        96  \\\n",
       "0  0.178076  0.024288 -0.016427  0.016274 -0.063890 -0.139000  0.106942   \n",
       "1  0.148205  0.034288 -0.035965  0.033020 -0.022341 -0.099260  0.208515   \n",
       "2  0.143718  0.040915 -0.018511  0.028187 -0.019662 -0.100381  0.110346   \n",
       "3  0.119139  0.043489 -0.012940  0.025341  0.014246 -0.057026  0.168648   \n",
       "4  0.142472  0.040177 -0.026639  0.038900 -0.023089 -0.104056  0.151252   \n",
       "\n",
       "         97        98        99  \n",
       "0  0.036321 -0.016587 -0.087598  \n",
       "1  0.149304 -0.057306 -0.186135  \n",
       "2  0.068416 -0.044475 -0.107071  \n",
       "3  0.102968 -0.070157 -0.163152  \n",
       "4  0.110283 -0.047110 -0.138467  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_arr = np.zeros((sentences.shape[0], 100))\n",
    "\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    word2vec_arr[i,:] = np.mean([model[lemma] for lemma in sentence], axis=0)\n",
    "\n",
    "word2vec_arr = pd.DataFrame(word2vec_arr)\n",
    "w2v_sentences = pd.concat([sentences[[\"-sentiment-\", \"text\"]],word2vec_arr], axis=1)\n",
    "w2v_sentences.dropna(inplace=True)\n",
    "\n",
    "w2v_sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Word2Vec data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "Training set score: 0.5386012715712988\n",
      "\n",
      "Test set score: 0.5592643051771117\n",
      "----------------------Random Forest Scores----------------------\n",
      "Training set score: 0.9909173478655767\n",
      "\n",
      "Test set score: 0.5504087193460491\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "Training set score: 0.8578564940962761\n",
      "\n",
      "Test set score: 0.5497275204359673\n"
     ]
    }
   ],
   "source": [
    "Y = w2v_sentences['-sentiment-']\n",
    "X = np.array(w2v_sentences.drop(['text','-sentiment-'], 1))\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)\n",
    "\n",
    "# Models\n",
    "lr = LogisticRegression()\n",
    "rfc = RandomForestClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "rfc.fit(X_train, y_train)\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "print(\"----------------------Logistic Regression Scores----------------------\")\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Random Forest Scores----------------------\")\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
    "print('Training set score:', gbc.score(X_train, y_train))\n",
    "print('\\nTest set score:', gbc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "text-nlp-challenge.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
