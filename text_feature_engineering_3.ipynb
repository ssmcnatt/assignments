{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zyMBrDwJnyd3"
   },
   "source": [
    "## Train your own word2vec representations, as you did in the first example in this checkpoint. However, you need to experiment with the hyperparameters of the vectorization step. Modify the hyperparameters and run the classification models again. Can you wrangle any improvements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "QajclIwjnyeA",
    "outputId": "32c6c2ae-4596-4789-e129-3d1eb92331b9",
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import gensim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# nltk.download('gutenberg')\n",
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qt6pE-RAnyeE"
   },
   "source": [
    "Before moving on to vectorizing the text, you need to clean your data. You can use the same cleaning codes as in the previous checkpoints, because you're using the same documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GStl5G-gnyeF"
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation that spaCy doesn't\n",
    "    # recognize: the double dash --. Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fYGkgFujnyeI"
   },
   "outputs": [],
   "source": [
    "# Load and clean the data\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xw1gGshfnyeJ"
   },
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take some time.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "nVRmVxehnyeL",
    "outputId": "0aa6d425-1bc2-45a2-875a-4ca7090ed647"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   author\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                                      (Oh, dear, !)  Carroll"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one DataFrame\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns = [\"text\", \"author\"])\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wf7Wk2NwnyeN"
   },
   "outputs": [],
   "source": [
    "# Get rid of stop words and punctuation,\n",
    "# and lemmatize the tokens\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    sentences.loc[i, \"text\"] = [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DNPcJeLmnyeP"
   },
   "source": [
    "Now, you're ready to vectorize your words using word2vec. For this purpose, use `Word2Vec` from Gensim's `models` module. The `Word2Vec` class has several parameters. Set the following parameters:\n",
    "\n",
    "* `workers=4`: Set the number of threads to run in parallel to 4 (which makes sense if your computer has available computing units).\n",
    "* `min_count=1`: Set the minimum word count threshold to 1.\n",
    "* `window=6`: Set the number of words around the target word to consider to 6.\n",
    "* `sg=0`: Use CBOW because your corpus is small.\n",
    "* `sample=1e-3`: Penalize frequent words.\n",
    "* `size=100`: Set the word vector length to 100.\n",
    "* `hs=1`: Use hierarchical softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CaHMx8TfnyeQ"
   },
   "outputs": [],
   "source": [
    "# Train word2vec on the sentences\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences[\"text\"],\n",
    "    workers=4,\n",
    "    min_count=0.5,\n",
    "    window=12,\n",
    "    sg=0,\n",
    "    sample=0.001,\n",
    "    size=100,\n",
    "    hs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_39am1FnyeT"
   },
   "source": [
    "Before jumping into the machine-learning model for prediction, play with the word2vec word representation that you just trained. Specifically, look into the following:\n",
    "\n",
    "* The first five words that are closer to `lady`\n",
    "* The word that doesn't fit in this list: `dad`, `dinner`, `mom`, `aunt`, `uncle`\n",
    "* The similarity score of `woman` and `man`\n",
    "* The similarity score of `horse` and `cat`\n",
    "\n",
    "Note that all of the above calculations are based on the word2vec representations of the words that you just trained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "qRN8FY92nyeT",
    "outputId": "e9ed3e27-03bd-4c11-c646-fd5220d7a465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('life', 0.9990490078926086), ('young', 0.9986709952354431), ('understand', 0.9986592531204224), ('board', 0.998489499092102), ('concern', 0.9983574151992798)]\n",
      "dinner\n",
      "0.99772704\n",
      "0.9253813\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['lady', 'man'], negative=['woman'], topn=5))\n",
    "print(model.doesnt_match(\"dad dinner mom aunt uncle\".split()))\n",
    "print(model.similarity('woman', 'man'))\n",
    "print(model.similarity('horse', 'cat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XAPrzFiEnyeV"
   },
   "source": [
    "Well, the results make sense to some degree, but it's obvious that your representations aren't perfect. This is because your corpus is small. To get more meaningful results, you need to train word2vec representations using much larger corpora.\n",
    "\n",
    "Now, create your numerical features using the word2vec representations of the words. In the following, get the word2vec vectors of each word in a sentence. Then take the average of all the vectors in the high dimensional space (in your case, it's 100). So, as a result, you'll have a vector of 100 dimensions as the feature for a sentence. You can then use each dimension as a separate feature—which means that you'll have 100 numerical features in your final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "colab_type": "code",
    "id": "5avMNgVXnyeW",
    "outputId": "f3d3aa1c-e085-4ffe-de40-4a057cc21608",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[Alice, begin, tired, sit, sister, bank, have,...</td>\n",
       "      <td>-0.286330</td>\n",
       "      <td>0.189602</td>\n",
       "      <td>0.040008</td>\n",
       "      <td>0.504310</td>\n",
       "      <td>0.122309</td>\n",
       "      <td>-0.015341</td>\n",
       "      <td>-0.251262</td>\n",
       "      <td>-0.183050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108080</td>\n",
       "      <td>0.218905</td>\n",
       "      <td>-0.378288</td>\n",
       "      <td>-0.071109</td>\n",
       "      <td>-0.051211</td>\n",
       "      <td>-0.158687</td>\n",
       "      <td>0.090778</td>\n",
       "      <td>0.460301</td>\n",
       "      <td>-0.224636</td>\n",
       "      <td>0.050887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[consider, mind, hot, day, feel, sleepy, stupi...</td>\n",
       "      <td>-0.246307</td>\n",
       "      <td>0.180035</td>\n",
       "      <td>0.012813</td>\n",
       "      <td>0.439538</td>\n",
       "      <td>0.096807</td>\n",
       "      <td>-0.022329</td>\n",
       "      <td>-0.224655</td>\n",
       "      <td>-0.147564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113968</td>\n",
       "      <td>0.203870</td>\n",
       "      <td>-0.314791</td>\n",
       "      <td>-0.076602</td>\n",
       "      <td>-0.035280</td>\n",
       "      <td>-0.114029</td>\n",
       "      <td>0.101395</td>\n",
       "      <td>0.379345</td>\n",
       "      <td>-0.168127</td>\n",
       "      <td>0.027302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[remarkable, Alice, think, way, hear, Rabbit]</td>\n",
       "      <td>-0.347402</td>\n",
       "      <td>0.232404</td>\n",
       "      <td>0.048664</td>\n",
       "      <td>0.612269</td>\n",
       "      <td>0.152939</td>\n",
       "      <td>-0.036372</td>\n",
       "      <td>-0.301442</td>\n",
       "      <td>-0.229480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129820</td>\n",
       "      <td>0.250894</td>\n",
       "      <td>-0.446079</td>\n",
       "      <td>-0.086098</td>\n",
       "      <td>-0.082392</td>\n",
       "      <td>-0.183035</td>\n",
       "      <td>0.096149</td>\n",
       "      <td>0.539733</td>\n",
       "      <td>-0.273010</td>\n",
       "      <td>0.060775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[oh, dear]</td>\n",
       "      <td>-0.304499</td>\n",
       "      <td>0.233723</td>\n",
       "      <td>0.049377</td>\n",
       "      <td>0.521712</td>\n",
       "      <td>0.138025</td>\n",
       "      <td>-0.032585</td>\n",
       "      <td>-0.274533</td>\n",
       "      <td>-0.226620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132914</td>\n",
       "      <td>0.198088</td>\n",
       "      <td>-0.328660</td>\n",
       "      <td>-0.073778</td>\n",
       "      <td>-0.035742</td>\n",
       "      <td>-0.124326</td>\n",
       "      <td>0.092076</td>\n",
       "      <td>0.438679</td>\n",
       "      <td>-0.256180</td>\n",
       "      <td>0.018199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[oh, dear]</td>\n",
       "      <td>-0.304499</td>\n",
       "      <td>0.233723</td>\n",
       "      <td>0.049377</td>\n",
       "      <td>0.521712</td>\n",
       "      <td>0.138025</td>\n",
       "      <td>-0.032585</td>\n",
       "      <td>-0.274533</td>\n",
       "      <td>-0.226620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132914</td>\n",
       "      <td>0.198088</td>\n",
       "      <td>-0.328660</td>\n",
       "      <td>-0.073778</td>\n",
       "      <td>-0.035742</td>\n",
       "      <td>-0.124326</td>\n",
       "      <td>0.092076</td>\n",
       "      <td>0.438679</td>\n",
       "      <td>-0.256180</td>\n",
       "      <td>0.018199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    author                                               text         0  \\\n",
       "0  Carroll  [Alice, begin, tired, sit, sister, bank, have,... -0.286330   \n",
       "1  Carroll  [consider, mind, hot, day, feel, sleepy, stupi... -0.246307   \n",
       "2  Carroll      [remarkable, Alice, think, way, hear, Rabbit] -0.347402   \n",
       "3  Carroll                                         [oh, dear] -0.304499   \n",
       "4  Carroll                                         [oh, dear] -0.304499   \n",
       "\n",
       "          1         2         3         4         5         6         7  ...  \\\n",
       "0  0.189602  0.040008  0.504310  0.122309 -0.015341 -0.251262 -0.183050  ...   \n",
       "1  0.180035  0.012813  0.439538  0.096807 -0.022329 -0.224655 -0.147564  ...   \n",
       "2  0.232404  0.048664  0.612269  0.152939 -0.036372 -0.301442 -0.229480  ...   \n",
       "3  0.233723  0.049377  0.521712  0.138025 -0.032585 -0.274533 -0.226620  ...   \n",
       "4  0.233723  0.049377  0.521712  0.138025 -0.032585 -0.274533 -0.226620  ...   \n",
       "\n",
       "         90        91        92        93        94        95        96  \\\n",
       "0  0.108080  0.218905 -0.378288 -0.071109 -0.051211 -0.158687  0.090778   \n",
       "1  0.113968  0.203870 -0.314791 -0.076602 -0.035280 -0.114029  0.101395   \n",
       "2  0.129820  0.250894 -0.446079 -0.086098 -0.082392 -0.183035  0.096149   \n",
       "3  0.132914  0.198088 -0.328660 -0.073778 -0.035742 -0.124326  0.092076   \n",
       "4  0.132914  0.198088 -0.328660 -0.073778 -0.035742 -0.124326  0.092076   \n",
       "\n",
       "         97        98        99  \n",
       "0  0.460301 -0.224636  0.050887  \n",
       "1  0.379345 -0.168127  0.027302  \n",
       "2  0.539733 -0.273010  0.060775  \n",
       "3  0.438679 -0.256180  0.018199  \n",
       "4  0.438679 -0.256180  0.018199  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_arr = np.zeros((sentences.shape[0],100))\n",
    "\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    word2vec_arr[i,:] = np.mean([model[lemma] for lemma in sentence], axis=0)\n",
    "\n",
    "word2vec_arr = pd.DataFrame(word2vec_arr)\n",
    "sentences = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr], axis=1)\n",
    "sentences.dropna(inplace=True)\n",
    "\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kK3W2MKXnyeY"
   },
   "source": [
    "## Word2vec in action\n",
    "\n",
    "Notice that you now have a dataset where the columns named from *0* to *99* are the features that you'll use in the following models. Use the same models that you built in the previous checkpoints to predict the author of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "6-repE9SnyeZ",
    "outputId": "d2e94261-0cf5-423f-f6e0-981048de397d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "Training set score: 0.8113207547169812\n",
      "\n",
      "Test set score: 0.8073170731707318\n",
      "----------------------Random Forest Scores----------------------\n",
      "Training set score: 0.9934938191281718\n",
      "\n",
      "Test set score: 0.8312195121951219\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "Training set score: 0.8845152895250488\n",
      "\n",
      "Test set score: 0.8346341463414634\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = sentences['author']\n",
    "X = np.array(sentences.drop(['text','author'], 1))\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)\n",
    "\n",
    "# Models\n",
    "lr = LogisticRegression()\n",
    "rfc = RandomForestClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "rfc.fit(X_train, y_train)\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "print(\"----------------------Logistic Regression Scores----------------------\")\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Random Forest Scores----------------------\")\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
    "print('Training set score:', gbc.score(X_train, y_train))\n",
    "print('\\nTest set score:', gbc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "6.feature_engineering_3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "96px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
